# scraper.py (Ä°lk Versiyon)

Bu dosya, projenin ilk safhasÄ±nda kullanÄ±lan temel scraper.py kodunu arÅŸivlemek iÃ§in memory-bank klasÃ¶rÃ¼ne taÅŸÄ±nmÄ±ÅŸtÄ±r. Kodun tamamÄ± aÅŸaÄŸÄ±dadÄ±r:

```python
# -*- coding: utf-8 -*-

"""
E-commerce Product Scraper
---
Bu betik, bir e-ticaret web sitesinden Ã¼rÃ¼n bilgilerini (isim ve fiyat)
Ã§ekmek iÃ§in tasarlanmÄ±ÅŸtÄ±r. `requests` kÃ¼tÃ¼phanesi ile web sitesinin HTML
iÃ§eriÄŸini alÄ±r ve `BeautifulSoup` ile bu iÃ§eriÄŸi ayrÄ±ÅŸtÄ±rarak istenen
verileri Ã§Ä±karÄ±r.

EÄŸitim amaÃ§lÄ± olarak hazÄ±rlanmÄ±ÅŸtÄ±r ve temel web kazÄ±ma prensiplerini
gÃ¶stermeyi hedefler.

KullanÄ±m:
    python scraper.py
"""

import requests
from bs4 import BeautifulSoup

# --- SABÄ°TLER ---

import sys

# Hedef web sitesinin URL'si.
# Script baÅŸlatÄ±lÄ±rken kullanÄ±cÄ±dan alÄ±nacak. VarsayÄ±lanlar: Hepsiburada, Trendyol, N11
DEFAULT_SITES = {
    "1": ("Hepsiburada", "https://www.hepsiburada.com/"),
    "2": ("Trendyol", "https://www.trendyol.com/"),
    "3": ("N11", "https://www.n11.com/"),
}

# TarayÄ±cÄ± taklidi yapmak iÃ§in HTTP baÅŸlÄ±klarÄ±. BazÄ± siteler, otomatik botlarÄ±
# engellemek iÃ§in User-Agent kontrolÃ¼ yapar.
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
}

# --- DEMO VERÄ°LERÄ° ---

# Web sitesine ulaÅŸÄ±lamadÄ±ÄŸÄ±nda veya veri Ã§ekme baÅŸarÄ±sÄ±z olduÄŸunda
# gÃ¶sterilecek yedek veriler.
DEMO_PRODUCTS = [
    {"name": "Ã–rnek ÃœrÃ¼n 1: Laptop", "price": "15.000 TL"},
    {"name": "Ã–rnek ÃœrÃ¼n 2: AkÄ±llÄ± Telefon", "price": "8.000 TL"},
    {"name": "Ã–rnek ÃœrÃ¼n 3: KulaklÄ±k", "price": "1.200 TL"},
]

# --- ANA FONKSÄ°YONLAR ---

def fetch_page_content(url, headers):
    """
    Verilen URL'ye bir GET isteÄŸi gÃ¶nderir ve sayfanÄ±n HTML iÃ§eriÄŸini dÃ¶ndÃ¼rÃ¼r.

    Args:
        url (str): Ä°stek gÃ¶nderilecek web sitesi URL'si.
        headers (dict): HTTP isteÄŸi iÃ§in gÃ¶nderilecek baÅŸlÄ±klar.

    Returns:
        str: SayfanÄ±n HTML iÃ§eriÄŸi. BaÅŸarÄ±sÄ±z olursa None dÃ¶ner.
    """
    try:
        response = requests.get(url, headers=headers, timeout=10)
        # HTTP 200 OK durum kodunu kontrol et
        response.raise_for_status()
        return response.text
    except requests.exceptions.HTTPError as e:
        # HTTP hata kodlarÄ±na gÃ¶re Ã¶zel mesajlar
        status_code = e.response.status_code
        if status_code == 403:
            print(f"\nâŒ HATA 403 (Forbidden): Web sitesi eriÅŸimi engelledi.")
            print("   Sebep: Site, isteÄŸinizi bot olarak algÄ±ladÄ± ve eriÅŸimi engelledi.")
            print("   Ã‡Ã¶zÃ¼m: Bu siteler, Cloudflare, CAPTCHA veya diÄŸer anti-bot sistemleri kullanÄ±yor.")
            print("   Not: Basit HTTP istekleri ile bu sitelere eriÅŸim mÃ¼mkÃ¼n deÄŸildir.")
            print("   Alternatif: Selenium gibi tarayÄ±cÄ± otomasyon araÃ§larÄ± gerekebilir.")
        elif status_code == 404:
            print(f"\nâŒ HATA 404 (Not Found): Sayfa bulunamadÄ±.")
            print("   URL'yi kontrol edin.")
        elif status_code == 429:
            print(f"\nâŒ HATA 429 (Too Many Requests): Ã‡ok fazla istek gÃ¶nderildi.")
            print("   LÃ¼tfen bir sÃ¼re bekleyip tekrar deneyin.")
        elif status_code >= 500:
            print(f"\nâŒ HATA {status_code} (Server Error): Sunucu hatasÄ±.")
            print("   Web sitesinin sunucusunda bir sorun var.")
        else:
            print(f"\nâŒ HTTP HatasÄ± {status_code}: {e}")
        return None
    except requests.exceptions.ConnectionError:
        print(f"\nâŒ BaÄŸlantÄ± HatasÄ±: Ä°nternet baÄŸlantÄ±nÄ±zÄ± kontrol edin veya URL geÃ§ersiz.")
        return None
    except requests.exceptions.Timeout:
        print(f"\nâŒ Zaman AÅŸÄ±mÄ± HatasÄ±: Sunucu 10 saniye iÃ§inde yanÄ±t vermedi.")
        return None
    except requests.RequestException as e:
        print(f"\nâŒ Beklenmeyen Hata: {type(e).__name__} - {e}")
        return None

def parse_products(html_content):
    """
    HTML iÃ§eriÄŸini ayrÄ±ÅŸtÄ±rÄ±r ve Ã¼rÃ¼n bilgilerini (isim, fiyat) Ã§Ä±karÄ±r.

    Args:
        html_content (str): AyrÄ±ÅŸtÄ±rÄ±lacak HTML iÃ§eriÄŸi.

    Returns:
        list: Her biri isim ve fiyat iÃ§eren sÃ¶zlÃ¼klerden oluÅŸan bir liste.
              BaÅŸarÄ±sÄ±z olursa boÅŸ bir liste dÃ¶ner.
    """
    if not html_content:
        return []

    soup = BeautifulSoup(html_content, "lxml")
    products = []

    # Ã–NEMLÄ°: AÅŸaÄŸÄ±daki seÃ§iciler, hedef web sitesinin HTML yapÄ±sÄ±na gÃ¶re
    # gÃ¼ncellenmelidir. Bu seÃ§iciler sadece birer Ã¶rnektir.
    # 'product-item' -> her bir Ã¼rÃ¼nÃ¼n ana kapsayÄ±cÄ±sÄ±
    # 'product-title' -> Ã¼rÃ¼n isminin bulunduÄŸu etiket
    # 'product-price' -> fiyatÄ±n bulunduÄŸu etiket
    product_elements = soup.select(".product-item")

    for element in product_elements[:5]: # Ä°lk 5 Ã¼rÃ¼nÃ¼ al
        name_element = element.select_one(".product-title")
        price_element = element.select_one(".product-price")

        if name_element and price_element:
            name = name_element.get_text(strip=True)
            price = price_element.get_text(strip=True)
            products.append({"name": name, "price": price})

    return products

def display_products(product_list):
    """
    ÃœrÃ¼n listesini formatlÄ± bir ÅŸekilde terminalde gÃ¶sterir.

    Args:
        product_list (list): GÃ¶sterilecek Ã¼rÃ¼nlerin listesi.
    """
    print("\n" + "="*50)
    if not product_list:
        print("GÃ¶sterilecek Ã¼rÃ¼n bulunamadÄ±.")
    else:
        for i, product in enumerate(product_list, 1):
            print(f"{i}. ÃœrÃ¼n: {product['name']}")
            print(f"   Fiyat: {product['price']}")
            print("-"*50)

# --- ANA PROGRAM AKIÅI ---


def main():
    """
    Ana program fonksiyonu.
    """
    print("ğŸ›ï¸  E-ticaret ÃœrÃ¼n KazÄ±yÄ±cÄ±")
    print("="*50)
    print("Hangi siteden Ã¼rÃ¼n bilgisi kazÄ±nsÄ±n?")
    for key, (name, url) in DEFAULT_SITES.items():
        print(f"{key}) {name} ({url})")
    print("4) Manuel giriÅŸ")
    print("BoÅŸ bÄ±rakÄ±lÄ±rsa 1 (Hepsiburada) seÃ§ilir.")
    secim = input("SeÃ§iminiz (1/2/3/4): ").strip()
    if secim == "2":
        user_url = DEFAULT_SITES["2"][1]
    elif secim == "3":
        user_url = DEFAULT_SITES["3"][1]
    elif secim == "4":
        user_url = input("LÃ¼tfen bir e-ticaret Ã¼rÃ¼n listeleme sayfasÄ± URL'si girin: ").strip()
        if not user_url:
            print("GeÃ§erli bir URL girilmedi, varsayÄ±lan olarak Hepsiburada seÃ§ildi.")
            user_url = DEFAULT_SITES["1"][1]
    else:
        user_url = DEFAULT_SITES["1"][1]
    print(f"\n'{user_url}' adresinden Ã¼rÃ¼nler Ã§ekiliyor...")

    html = fetch_page_content(user_url, HEADERS)
    products = parse_products(html)

    if not products:
        print("\nUyarÄ±: GerÃ§ek veriler Ã§ekilemedi. Demo verileri gÃ¶steriliyor.")
        products_to_display = DEMO_PRODUCTS
    else:
        products_to_display = products

    display_products(products_to_display)


if __name__ == "__main__":
    main()
```
