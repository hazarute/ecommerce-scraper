# E-commerce Product Scraper ğŸ›’

[![Python Version](https://img.shields.io/badge/python-3.8%2B-blue)](https://www.python.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Status](https://img.shields.io/badge/status-active%20development-orange.svg)](https://github.com/hazarute/ecommerce-scraper)

**[TÃ¼rkÃ§e AÃ§Ä±klama Ä°Ã§in AÅŸaÄŸÄ±ya Ä°nin](#tr-e-ticaret-Ã¼rÃ¼n-kazÄ±yÄ±cÄ±-)**

---

## ğŸŒŸ About The Project

A **modular, extensible, and production-ready** Python web scraper designed to extract product information from Turkish e-commerce websites. The project demonstrates both basic (Requests + BeautifulSoup) and advanced (Selenium) web scraping techniques within a clean, scalable architecture.

**Current Phase:** Transitioning from CLI-based scraper to **Streamlit UI with Plugin Architecture**.

### Key Goals
- âœ… Educational: Clear, well-documented code for learning web scraping principles
- âœ… Production-Ready: Modular design with factory pattern, strategy pattern, and plugin discovery
- âœ… Flexible: Support for multiple scraping strategies (Requests, Selenium, custom plugins)
- âœ… Extensible: Users can add custom scrapers via `custom_plugins/` without modifying core code
- âœ… Maintainable: Memory Bank system ensures transparent project state and decisions

### Built With
- [Python](https://www.python.org/) 3.8+
- [Streamlit](https://streamlit.io/) â€” Modern UI framework
- [Requests](https://docs.python-requests.org/) â€” Lightweight HTTP client
- [BeautifulSoup4](https://www.crummy.com/software/BeautifulSoup/) â€” HTML parsing
- [Selenium](https://www.selenium.dev/) â€” Browser automation for dynamic content
- [webdriver-manager](https://github.com/SergeyPirogov/webdriver_manager) â€” Automatic WebDriver management

---

## ğŸš€ Features

### Core Features
- **Hybrid Scraping Modes**  
  Choose between lightweight (Requests + BeautifulSoup) for fast, simple sites or Selenium for anti-bot protection and dynamic content.

- **Streamlit Web Interface**  
  Modern UI for site selection, scraping mode configuration, and real-time job monitoring. No CLI needed.

- **Plugin Architecture**  
  Add custom scrapers dynamically via `custom_plugins/` folder. Each plugin follows a simple contract: `metadata` dict + `run(url, config) â†’ List[Dict]`.

- **Site-Specific Configuration**  
  All CSS selectors and site-specific settings centralized in `config/sites_config.json`. Easy to extend for new sites.

- **Site-Specific Parse Functions**  
  - **Hepsiburada:** JSON-LD parsing for maximum reliability
  - **N11 & Trendyol:** Custom CSS selectors and parsing logic optimized per site

- **Automatic Selector Detection**  
  Manual URLs are auto-detected and mapped to the correct site and selectors.

- **Demo Data Fallback**  
  If scraping fails due to anti-bot measures or selector issues, demo product data is displayed for testing.

- **Export Options**  
  Results exported as CSV or JSON with automatic timestamp naming (`site_YYYYMMDD_HHMMSS.json`).

- **Debug Mode**  
  Saved page sources (`data/page_sources/`) and detailed logs for troubleshooting.

- **Memory Bank System**  
  Project context, architectural decisions, and progress tracked in `/.memory-bank/` for transparency and maintainability across sessions.

---

## ğŸ“¦ Installation

### Prerequisites
- **Python 3.8 or higher**
- **pip** (Python package installer)
- **Git** (for cloning)

### Steps

1. **Clone the repository**
   ```bash
   git clone https://github.com/hazarute/ecommerce-scraper.git
   cd ecommerce-scraper
   ```

2. **Create a virtual environment** (recommended)
   ```bash
   # Windows (PowerShell)
   python -m venv .venv
   .\.venv\Scripts\Activate.ps1
   
   # macOS/Linux
   python3 -m venv .venv
   source .venv/bin/activate
   ```

3. **Install dependencies**
   ```bash
   pip install -r requirements.txt
   ```

4. **Verify installation**
   ```bash
   streamlit run app.py
   # OR (legacy CLI mode)
   python scraper.py
   ```

---

## ğŸ’» Usage

### **Modern Way: Streamlit UI** (Recommended)
```bash
streamlit run app.py
```
- Open your browser at `http://localhost:8501`
- Select site, scraping mode, and configuration
- View results in real-time
- Export as CSV or JSON

### **Legacy CLI Mode** (Still Supported)
```bash
python scraper.py
```
- Follow the interactive prompts
- Select scraping mode (Requests/Selenium)
- Choose site (Hepsiburada, Trendyol, N11) or enter manual URL
- Results displayed in terminal

### **Selenium Headless Mode**
When Selenium is selected, you'll be prompted:
```
Selenium iÃ§in headless modda Ã§alÄ±ÅŸtÄ±rÄ±lsÄ±n mÄ±? (E/h):
```
- Press **Enter** or type **E** for headless (no browser window) â€” Recommended for servers
- Type **h** to see the browser window during scraping

### How It Works

1. **Mode Selection:** Choose between Requests (fast, basic) or Selenium (advanced, anti-bot)
2. **Site Selection:** Pick from preset sites or enter a custom URL
3. **Auto-Detection:** Site and selectors automatically detected from URL
4. **Data Extraction:** Site-specific parse function extracts product names, prices, URLs
5. **Fallback:** Demo data shown if scraping fails
6. **Export:** Results saved as CSV/JSON with timestamp

### Example Output (Terminal)
```
ğŸ›ï¸  E-commerce Product Scraper
==================================================
Select scraping mode:
1) Requests (fast, basic)
2) Selenium (advanced, anti-bot)
Default is 1 (Requests).
Mode (1/2): 2

Which site to scrape product info from?
1) Hepsiburada (https://www.hepsiburada.com/)
2) Trendyol (https://www.trendyol.com/)
3) N11 (https://www.n11.com/)
4) Manual entry
Default is 1 (Hepsiburada).
Selection (1/2/3/4): 1
Fetching products from 'https://www.hepsiburada.com/' ...

1. Product: iPhone 14 Pro
   Price: 45.999 â‚º
   URL: https://www.hepsiburada.com/...
--------------------------------------------------
2. Product: Samsung Galaxy S23
   Price: 32.499 â‚º
   URL: https://www.hepsiburada.com/...
--------------------------------------------------
```

---

## ğŸ“ Project Structure

```
e-commerce-product-scraper/
â”œâ”€ app.py                      # Streamlit UI (modern interface)
â”œâ”€ scraper.py                  # CLI entry point (legacy, maintained)
â”œâ”€
â”œâ”€ core/                       # Core scraping engine (under development)
â”‚  â”œâ”€ engine.py               # Job orchestration & plugin discovery
â”‚  â””â”€ scrapers/               # Core scraper modules
â”‚     â”œâ”€ __init__.py
â”‚     â”œâ”€ base_scraper.py      # Abstract base class
â”‚     â”œâ”€ requests_scraper.py  # Requests + BeautifulSoup strategy
â”‚     â””â”€ selenium_scraper.py  # Selenium strategy
â”‚
â”œâ”€ scrapers/                  # Current scraper modules (being migrated to core/)
â”‚  â”œâ”€ __init__.py
â”‚  â”œâ”€ base_scraper.py
â”‚  â”œâ”€ requests_scraper.py
â”‚  â””â”€ selenium_scraper.py
â”‚
â”œâ”€ custom_plugins/            # User-defined scrapers (plugin architecture)
â”‚  â”œâ”€ _template.py           # Example plugin template
â”‚  â”œâ”€ README.md              # Plugin development guide
â”‚  â””â”€ [user plugins here]
â”‚
â”œâ”€ utils/                     # Utility modules (under development)
â”‚  â”œâ”€ exporters.py           # CSV/JSON export functions
â”‚  â””â”€ fileops.py             # File operations & management
â”‚
â”œâ”€ config/
â”‚  â””â”€ sites_config.json      # Site-specific selectors & settings
â”‚
â”œâ”€ data/
â”‚  â”œâ”€ page_sources/          # Saved HTML snapshots (debug)
â”‚  â”œâ”€ exports/               # CSV/JSON export output
â”‚  â””â”€ logs/                  # Execution logs
â”‚
â”œâ”€ scripts/
â”‚  â””â”€ legacy_scraper.py      # Backup of original script (if needed)
â”‚
â”œâ”€ .memory-bank/             # Project brain (AI-Driven Development)
â”‚  â”œâ”€ projectbrief.md        # Project definition & scope
â”‚  â”œâ”€ productContext.md      # Why this project exists & target users
â”‚  â”œâ”€ activeContext.md       # Current mental state & active decisions
â”‚  â”œâ”€ systemPatterns.md      # Architecture & design patterns
â”‚  â”œâ”€ techContext.md         # Technology stack & setup
â”‚  â””â”€ progress.md            # Task status & known issues
â”‚
â”œâ”€ requirements.txt          # Python dependencies
â”œâ”€ .env.example              # Environment variables template
â”œâ”€ .gitignore               # Git ignore rules
â”œâ”€ LICENSE                  # MIT License
â””â”€ README.md               # This file
```

---

## ğŸ”§ Configuration

### Site Configuration (`config/sites_config.json`)
Define CSS selectors and settings for each e-commerce site:

```json
{
  "hepsiburada": {
    "url": "https://www.hepsiburada.com/",
    "selectors": {
      "product_container": "...",
      "product_name": "...",
      "product_price": "..."
    },
    "parse_function": "parse_hepsiburada"
  },
  "trendyol": {
    "url": "https://www.trendyol.com/",
    "selectors": {...},
    "parse_function": "parse_trendyol"
  },
  "n11": {
    "url": "https://www.n11.com/",
    "selectors": {...},
    "parse_function": "parse_n11"
  }
}
```

### Environment Variables (`.env`)
For sensitive data like API keys or proxy settings:
```bash
# .env
PROXY_URL=http://proxy.example.com:8080
API_KEY=your_secret_key_here
DEBUG=True
```

---

## ğŸ§© Plugin Development

Users can extend the scraper with custom plugins without modifying core code.

### Plugin Template (`custom_plugins/_template.py`)
```python
# metadata: Required
metadata = {
    "name": "My Custom Scraper",
    "version": "1.0.0",
    "description": "Scrapes my custom site",
    "supported_sites": ["mysite.com"],
}

# run function: Required
def run(url: str, config: dict) -> list:
    """
    Args:
        url (str): Target URL to scrape
        config (dict): Configuration dict from UI
    
    Returns:
        list: List of product dictionaries
              [{
                  "title": "...",
                  "price": "...",
                  "url": "...",
                  ...
              }, ...]
    """
    # Your scraping logic here
    results = []
    # ... extract products ...
    return results
```

### Adding a Custom Plugin
1. Copy `custom_plugins/_template.py` to `custom_plugins/my_scraper.py`
2. Implement `metadata` and `run()` function
3. Restart Streamlit UI â€” plugin auto-discovered and available for selection

---

## ğŸ§ª Testing & Debugging

### Run Streamlit in Debug Mode
```bash
streamlit run app.py --logger.level=debug
```

### Check Saved Page Sources
Failed scrapes save HTML to `data/page_sources/` for analysis:
```bash
ls data/page_sources/
# hepsiburada_20251114_143022.html
# trendyol_20251114_144511.html
```

### Run Unit Tests (when available)
```bash
pytest tests/ -v
```

---

## âš ï¸ Important Disclaimer & Ethical Use

**This project is for educational and research purposes only.**

- **Respect Website Terms of Service:** Always review and respect the ToS of any website you scrape
- **Check `robots.txt`:** Before scraping any site, verify that scraping is allowed
- **Rate Limiting:** Avoid excessive requests to prevent overloading servers. Use delays between requests
- **Anti-Bot Measures:** Some sites use Cloudflare, CAPTCHA, or JS-based protection. Selenium + headless mode helps, but may still be blocked
- **Data Privacy:** Do not scrape personal or sensitive user data
- **Liability:** The authors are **not responsible** for misuse of this code or legal consequences arising from scraping activities

---

## ğŸ“Š Known Limitations & Risks

### Anti-Bot Protection
Some Turkish e-commerce sites use:
- Cloudflare protection
- CAPTCHA challenges
- JavaScript-based content loading

**Workarounds:** Selenium with headless mode, rotating proxies, request delays, User-Agent rotation

### Plugin Security (Production)
Custom plugins execute arbitrary code. For production:
- âœ… Validate plugin manifest (metadata)
- âœ… Check plugin dependencies
- âœ… Run plugins in isolated containers or sandboxes
- âœ… Code review before deployment

---

## ğŸ¤ Contributing

Contributions are welcome! Please follow these steps:

1. **Fork the repository**
   ```bash
   git clone https://github.com/yourusername/ecommerce-scraper.git
   cd ecommerce-scraper
   ```

2. **Create a feature branch**
   ```bash
   git checkout -b feature/YourAmazingFeature
   ```

3. **Make your changes** and commit
   ```bash
   git commit -m "Add YourAmazingFeature"
   ```

4. **Push to the branch**
   ```bash
   git push origin feature/YourAmazingFeature
   ```

5. **Open a Pull Request** on GitHub

For major changes, please open an issue first to discuss your proposal.

---

## ğŸ“ License

This project is licensed under the **MIT License**. See the [LICENSE](LICENSE) file for details.

---

## ğŸ“ Contact & Support

**Author:** Hazar Ãœte  
**Email:** hazarute@gmail.com  
**GitHub:** [hazarute](https://github.com/hazarute)  
**Project Link:** [ecommerce-scraper](https://github.com/hazarute/ecommerce-scraper)

---

## ğŸ™ Acknowledgments & References

- [Requests Library](https://docs.python-requests.org/) â€” HTTP client
- [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/) â€” HTML parsing
- [Selenium](https://www.selenium.dev/) â€” Browser automation
- [Streamlit](https://streamlit.io/) â€” UI framework
- Web scraping tutorials and educational resources

---

---

# TR: E-ticaret ÃœrÃ¼n KazÄ±yÄ±cÄ± ğŸ›’

[![Python SÃ¼rÃ¼mÃ¼](https://img.shields.io/badge/python-3.8%2B-blue)](https://www.python.org/)
[![Lisans: MIT](https://img.shields.io/badge/Lisans-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Durum](https://img.shields.io/badge/durum-aktif%20geli%C5%9Ftirme-orange.svg)](https://github.com/hazarute/ecommerce-scraper)

---

## ğŸŒŸ Proje HakkÄ±nda

TÃ¼rkÃ§e e-ticaret sitelerinden Ã¼rÃ¼n bilgisi Ã§ekebilen, **modÃ¼ler, geniÅŸletilebilir ve Ã¼retim-hazÄ±r** bir Python web kazÄ±yÄ±cÄ±sÄ±dÄ±r. Proje, temel (Requests + BeautifulSoup) ve geliÅŸmiÅŸ (Selenium) web kazÄ±ma tekniklerini temiz, Ã¶lÃ§eklenebilir bir mimari iÃ§inde gÃ¶sterir.

**Åu Anki Faz:** CLI tabanlÄ± kazÄ±yÄ±cÄ±dan **Streamlit UI + Plugin Mimarisine** geÃ§iÅŸ yapÄ±lÄ±yor.

### Ana Hedefler
- âœ… EÄŸitici: Web kazÄ±ma ilkelerini Ã¶ÄŸrenmek iÃ§in aÃ§Ä±k, iyi belgelenmiÅŸ kod
- âœ… Ãœretim-HazÄ±r: Factory pattern, Strategy pattern ve plugin discovery ile modÃ¼ler tasarÄ±m
- âœ… Esnek: Birden fazla kazÄ±ma stratejisi (Requests, Selenium, custom plugin'ler)
- âœ… GeniÅŸletilebilir: KullanÄ±cÄ±lar core kodu deÄŸiÅŸtirmeden `custom_plugins/` klasÃ¶rÃ¼ne kendi kazÄ±yÄ±cÄ±larÄ±nÄ± ekleyebilir
- âœ… BakÄ±mlanabilir: Bellek BankasÄ± sistemi projenin durumunu ve kararlarÄ±nÄ± ÅŸeffaf tutar

### KullanÄ±lan Teknolojiler
- [Python](https://www.python.org/) 3.8+
- [Streamlit](https://streamlit.io/) â€” Modern UI framework'Ã¼
- [Requests](https://docs.python-requests.org/) â€” Hafif HTTP istemcisi
- [BeautifulSoup4](https://www.crummy.com/software/BeautifulSoup/) â€” HTML ayrÄ±ÅŸtÄ±rmasÄ±
- [Selenium](https://www.selenium.dev/) â€” TarayÄ±cÄ± otomasyonu dinamik iÃ§erik iÃ§in
- [webdriver-manager](https://github.com/SergeyPirogov/webdriver_manager) â€” Otomatik WebDriver yÃ¶netimi

---

## ğŸš€ Ã–zellikler

### Temel Ã–zellikler
- **Hibrit KazÄ±ma ModlarÄ±**  
  HÄ±zlÄ±, basit siteler iÃ§in Requests + BeautifulSoup veya anti-bot korumasÄ± ve dinamik iÃ§erik iÃ§in Selenium seÃ§in.

- **Streamlit Web ArayÃ¼zÃ¼**  
  Site seÃ§imi, kazÄ±ma modu ayarlarÄ± ve gerÃ§ek zamanlÄ± gÃ¶rev izleme iÃ§in modern UI. CLI'ye ihtiyaÃ§ yok.

- **Plugin Mimarisi**  
  `custom_plugins/` klasÃ¶rÃ¼ne dinamik olarak kendi kazÄ±yÄ±cÄ±larÄ±nÄ±zÄ± ekleyin. Her plugin basit bir sÃ¶zleÅŸmeyi takip eder: `metadata` dict + `run(url, config) â†’ List[Dict]`.

- **Siteye Ã–zel YapÄ±landÄ±rma**  
  TÃ¼m CSS seÃ§iciler ve siteye Ã¶zel ayarlar `config/sites_config.json` iÃ§inde merkezi yÃ¶netilir. Yeni siteler iÃ§in kolay geniÅŸletilir.

- **Siteye Ã–zel Parse FonksiyonlarÄ±**  
  - **Hepsiburada:** JSON-LD ayrÄ±ÅŸtÄ±rmasÄ± maksimum gÃ¼venilirlik iÃ§in
  - **N11 & Trendyol:** Her site iÃ§in optimize edilmiÅŸ CSS seÃ§icileri ve Ã¶zel parse mantÄ±ÄŸÄ±

- **Otomatik SeÃ§ici AlgÄ±lamasÄ±**  
  Manuel URL'ler otomatik olarak doÄŸru site ve seÃ§icilere eÅŸlenir.

- **Demo Veri Yedekleme**  
  Anti-bot Ã¶lÃ§Ã¼leri veya seÃ§ici hatalarÄ± nedeniyle kazÄ±ma baÅŸarÄ±sÄ±z olursa, demo Ã¼rÃ¼n verisi gÃ¶rÃ¼ntÃ¼lenir.

- **Export SeÃ§enekleri**  
  SonuÃ§lar CSV veya JSON olarak otomatik timestamp adlandÄ±rmasÄ± ile Ã§Ä±kartÄ±lÄ±r (`site_YYYYMMDD_HHMMSS.json`).

- **Debug Modu**  
  Sorun giderme iÃ§in kaydedilen sayfa kaynaklarÄ± (`data/page_sources/`) ve detaylÄ± loglar.

- **Bellek BankasÄ± Sistemi**  
  Proje baÄŸlamÄ±, mimari kararlar ve ilerleme `/.memory-bank/` klasÃ¶rÃ¼nde tutulur. ÅeffaflÄ±k ve oturumlar arasÄ± bakÄ±mlanabilirlik iÃ§in tasarlanmÄ±ÅŸ.

---

## ğŸ“¦ Kurulum

### Ã–n Gereksinimler
- **Python 3.8 veya Ã¼zeri**
- **pip** (Python paket yÃ¼kleyicisi)
- **Git** (depo klonlamak iÃ§in)

### AdÄ±mlar

1. **Depoyu klonlayÄ±n**
   ```bash
   git clone https://github.com/hazarute/ecommerce-scraper.git
   cd ecommerce-scraper
   ```

2. **Sanal ortam oluÅŸturun** (Ã¶nerilir)
   ```bash
   # Windows (PowerShell)
   python -m venv .venv
   .\.venv\Scripts\Activate.ps1
   
   # macOS/Linux
   python3 -m venv .venv
   source .venv/bin/activate
   ```

3. **BaÄŸÄ±mlÄ±lÄ±klarÄ± yÃ¼kleyin**
   ```bash
   pip install -r requirements.txt
   ```

4. **Kurulumu doÄŸrulayÄ±n**
   ```bash
   streamlit run app.py
   # VEYA (eski CLI modu)
   python scraper.py
   ```

---

## ğŸ’» KullanÄ±m

### **Modern Yol: Streamlit UI** (Ã–nerilir)
```bash
streamlit run app.py
```
- TarayÄ±cÄ±nÄ±zda `http://localhost:8501` adresini aÃ§Ä±n
- Siteyi, kazÄ±ma modunu ve yapÄ±landÄ±rmayÄ± seÃ§in
- SonuÃ§larÄ± gerÃ§ek zamanlÄ± olarak gÃ¶rÃ¼ntÃ¼leyin
- CSV veya JSON olarak dÄ±ÅŸa aktarÄ±n

### **Eski CLI Modu** (Hala Destekleniyor)
```bash
python scraper.py
```
- EtkileÅŸimli istemi izleyin
- KazÄ±ma modunu seÃ§in (Requests/Selenium)
- Siteyi seÃ§in (Hepsiburada, Trendyol, N11) veya manuel URL girin
- SonuÃ§lar terminalde gÃ¶sterilir

### **Selenium Headless Modu**
Selenium seÃ§ildiÄŸinde ÅŸu ÅŸekilde sorulur:
```
Selenium iÃ§in headless modda Ã§alÄ±ÅŸtÄ±rÄ±lsÄ±n mÄ±? (E/h):
```
- **Enter** basÄ±n veya **E** yazÄ±n headless (gÃ¶rÃ¼nmez) mod iÃ§in â€” Sunucular iÃ§in Ã¶nerilir
- **h** yazÄ±n kazÄ±ma sÄ±rasÄ±nda tarayÄ±cÄ± penceresini gÃ¶rmek iÃ§in

### NasÄ±l Ã‡alÄ±ÅŸÄ±r?

1. **Mod SeÃ§imi:** Requests (hÄ±zlÄ±, temel) veya Selenium (geliÅŸmiÅŸ, anti-bot) seÃ§in
2. **Site SeÃ§imi:** Ã–nceden ayarlanmÄ±ÅŸ sitelerden biri seÃ§in veya manuel URL girin
3. **Otomatik AlgÄ±lama:** Site ve seÃ§iciler URL'den otomatik olarak algÄ±lanÄ±r
4. **Veri Ã‡ekme:** Siteye Ã¶zel parse fonksiyonu Ã¼rÃ¼n adlarÄ±nÄ±, fiyatlarÄ±nÄ±, URL'lerini Ã§Ä±kartÄ±r
5. **Yedekleme:** KazÄ±ma baÅŸarÄ±sÄ±z olursa demo verisi gÃ¶sterilir
6. **Export:** SonuÃ§lar timestamp ile CSV/JSON olarak kaydedilir

### Ã–rnek Ã‡Ä±ktÄ± (Terminal)
```
ğŸ›ï¸  E-ticaret ÃœrÃ¼n KazÄ±yÄ±cÄ±
==================================================
KazÄ±ma modu seÃ§in:
1) Requests (hÄ±zlÄ±, temel)
2) Selenium (geliÅŸmiÅŸ, anti-bot)
BoÅŸ bÄ±rakÄ±lÄ±rsa 1 (Requests) seÃ§ilir.
Mod (1/2): 2

Hangi siteden Ã¼rÃ¼n bilgisi kazÄ±nsÄ±n?
1) Hepsiburada (https://www.hepsiburada.com/)
2) Trendyol (https://www.trendyol.com/)
3) N11 (https://www.n11.com/)
4) Manuel giriÅŸ
BoÅŸ bÄ±rakÄ±lÄ±rsa 1 (Hepsiburada) seÃ§ilir.
SeÃ§iminiz (1/2/3/4): 1
'https://www.hepsiburada.com/' adresinden Ã¼rÃ¼nler Ã§ekiliyor...

1. ÃœrÃ¼n: iPhone 14 Pro
   Fiyat: 45.999 â‚º
   URL: https://www.hepsiburada.com/...
--------------------------------------------------
2. ÃœrÃ¼n: Samsung Galaxy S23
   Fiyat: 32.499 â‚º
   URL: https://www.hepsiburada.com/...
--------------------------------------------------
```

---

## ğŸ“ Proje YapÄ±sÄ±

```
e-commerce-product-scraper/
â”œâ”€ app.py                      # Streamlit UI (modern arayÃ¼z)
â”œâ”€ scraper.py                  # CLI giriÅŸ noktasÄ± (eski, hala destekleniyor)
â”œâ”€
â”œâ”€ core/                       # Temel kazÄ±ma motoru (geliÅŸtirme aÅŸamasÄ±nda)
â”‚  â”œâ”€ engine.py               # GÃ¶rev orkestrasyonu ve plugin discovery
â”‚  â””â”€ scrapers/               # Temel kazÄ±yÄ±cÄ± modÃ¼lleri
â”‚     â”œâ”€ __init__.py
â”‚     â”œâ”€ base_scraper.py      # Soyut temel sÄ±nÄ±f
â”‚     â”œâ”€ requests_scraper.py  # Requests + BeautifulSoup stratejisi
â”‚     â””â”€ selenium_scraper.py  # Selenium stratejisi
â”‚
â”œâ”€ scrapers/                  # Mevcut kazÄ±yÄ±cÄ± modÃ¼lleri (core/'e taÅŸÄ±nÄ±yor)
â”‚  â”œâ”€ __init__.py
â”‚  â”œâ”€ base_scraper.py
â”‚  â”œâ”€ requests_scraper.py
â”‚  â””â”€ selenium_scraper.py
â”‚
â”œâ”€ custom_plugins/            # KullanÄ±cÄ± tanÄ±mlÄ± kazÄ±yÄ±cÄ±lar (plugin mimarisi)
â”‚  â”œâ”€ _template.py           # Ã–rnek plugin ÅŸablonu
â”‚  â”œâ”€ README.md              # Plugin geliÅŸtirme rehberi
â”‚  â””â”€ [kullanÄ±cÄ± plugin'leri burada]
â”‚
â”œâ”€ utils/                     # YardÄ±mcÄ± modÃ¼ller (geliÅŸtirme aÅŸamasÄ±nda)
â”‚  â”œâ”€ exporters.py           # CSV/JSON export fonksiyonlarÄ±
â”‚  â””â”€ fileops.py             # Dosya iÅŸlemleri ve yÃ¶netimi
â”‚
â”œâ”€ config/
â”‚  â””â”€ sites_config.json      # Siteye Ã¶zel seÃ§iciler ve ayarlar
â”‚
â”œâ”€ data/
â”‚  â”œâ”€ page_sources/          # Kaydedilen HTML snapshot'larÄ± (debug)
â”‚  â”œâ”€ exports/               # CSV/JSON export Ã§Ä±ktÄ±larÄ±
â”‚  â””â”€ logs/                  # Ã‡alÄ±ÅŸtÄ±rma loglarÄ±
â”‚
â”œâ”€ scripts/
â”‚  â””â”€ legacy_scraper.py      # Orijinal scriptin yedek kopyasÄ± (gerekirse)
â”‚
â”œâ”€ .memory-bank/             # Proje beyni (AI-Driven Development)
â”‚  â”œâ”€ projectbrief.md        # Proje tanÄ±mÄ± ve kapsamÄ±
â”‚  â”œâ”€ productContext.md      # Projenin neden var olduÄŸu ve hedef kullanÄ±cÄ±lar
â”‚  â”œâ”€ activeContext.md       # Mevcut zihinsel durum ve aktif kararlar
â”‚  â”œâ”€ systemPatterns.md      # Mimari ve tasarÄ±m desenleri
â”‚  â”œâ”€ techContext.md         # Teknoloji stack'i ve kurulum
â”‚  â””â”€ progress.md            # GÃ¶rev durumu ve bilinen sorunlar
â”‚
â”œâ”€ requirements.txt          # Python baÄŸÄ±mlÄ±lÄ±klarÄ±
â”œâ”€ .env.example              # Ortam deÄŸiÅŸkenleri ÅŸablonu
â”œâ”€ .gitignore               # Git ignore kurallarÄ±
â”œâ”€ LICENSE                  # MIT LisansÄ±
â””â”€ README.md               # Bu dosya
```

---

## ğŸ”§ YapÄ±landÄ±rma

### Site YapÄ±landÄ±rmasÄ± (`config/sites_config.json`)
Her e-ticaret sitesi iÃ§in CSS seÃ§iciler ve ayarlarÄ± tanÄ±mlayÄ±n:

```json
{
  "hepsiburada": {
    "url": "https://www.hepsiburada.com/",
    "selectors": {
      "product_container": "...",
      "product_name": "...",
      "product_price": "..."
    },
    "parse_function": "parse_hepsiburada"
  },
  "trendyol": {
    "url": "https://www.trendyol.com/",
    "selectors": {...},
    "parse_function": "parse_trendyol"
  },
  "n11": {
    "url": "https://www.n11.com/",
    "selectors": {...},
    "parse_function": "parse_n11"
  }
}
```

### Ortam DeÄŸiÅŸkenleri (`.env`)
API anahtarlarÄ± veya proxy ayarlarÄ± gibi hassas veriler iÃ§in:
```bash
# .env
PROXY_URL=http://proxy.example.com:8080
API_KEY=gizli_anahtar
DEBUG=True
```

---

## ğŸ§© Plugin GeliÅŸtirme

KullanÄ±cÄ±lar core kodu deÄŸiÅŸtirmeden custom plugin'ler ile kazÄ±yÄ±cÄ±yÄ± geniÅŸletebilir.

### Plugin Åablonu (`custom_plugins/_template.py`)
```python
# metadata: Zorunlu
metadata = {
    "name": "Benim Custom KazÄ±yÄ±cÄ±",
    "version": "1.0.0",
    "description": "Benim custom sitemi kazÄ±r",
    "supported_sites": ["mysite.com"],
}

# run fonksiyonu: Zorunlu
def run(url: str, config: dict) -> list:
    """
    Args:
        url (str): KazÄ±ma yapÄ±lacak hedef URL
        config (dict): UI'den gelen yapÄ±landÄ±rma dict'i
    
    Returns:
        list: ÃœrÃ¼n sÃ¶zlÃ¼klerinin listesi
              [{
                  "title": "...",
                  "price": "...",
                  "url": "...",
                  ...
              }, ...]
    """
    # KazÄ±ma mantÄ±ÄŸÄ±nÄ±z burada
    results = []
    # ... Ã¼rÃ¼nleri Ã§Ä±kartÄ±n ...
    return results
```

### Custom Plugin Ekleme
1. `custom_plugins/_template.py` dosyasÄ±nÄ± `custom_plugins/my_scraper.py` olarak kopyalayÄ±n
2. `metadata` ve `run()` fonksiyonunu uygulayin
3. Streamlit UI'Ä± yeniden baÅŸlatÄ±n â€” plugin otomatik keÅŸfedilir ve seÃ§ilir hale gelir

---

## ğŸ§ª Test Etme ve Debug

### Streamlit Debug Modunda Ã‡alÄ±ÅŸtÄ±rÄ±n
```bash
streamlit run app.py --logger.level=debug
```

### Kaydedilen Sayfa KaynaklarÄ±nÄ± Kontrol Edin
BaÅŸarÄ±sÄ±z kazÄ±malar HTML'i `data/page_sources/` iÃ§ine kaydeder:
```bash
ls data/page_sources/
# hepsiburada_20251114_143022.html
# trendyol_20251114_144511.html
```

### Unit Test'leri Ã‡alÄ±ÅŸtÄ±rÄ±n (varsa)
```bash
pytest tests/ -v
```

---

## âš ï¸ Ã–nemli UyarÄ± ve Etik KullanÄ±m

**Bu proje sadece eÄŸitim ve araÅŸtÄ±rma amaÃ§larÄ± iÃ§indir.**

- **Web Sitesi Hizmet KoÅŸullarÄ±na Uyun:** Herhangi bir siteyi kazÄ±madan Ã¶nce Hizmet KoÅŸullarÄ±'nÄ± gÃ¶zden geÃ§irin ve uyun
- **`robots.txt` Kontrol Edin:** Herhangi bir siteyi kazÄ±madan Ã¶nce kazÄ±manÄ±n izin verilip verilmediÄŸini doÄŸrulayÄ±n
- **HÄ±z SÄ±nÄ±rlamasÄ±:** SunucularÄ± aÅŸÄ±rÄ± yÃ¼klememek iÃ§in aÅŸÄ±rÄ± istek gÃ¶ndermekten kaÃ§Ä±nÄ±n. Ä°stekler arasÄ±nda gecikmeler kullanÄ±n
- **Anti-Bot Ã–lÃ§Ã¼leri:** BazÄ± siteler Cloudflare, CAPTCHA veya JS tabanlÄ± koruma kullanÄ±r. Selenium + headless mod yardÄ±mcÄ± olur, ancak yine de engellenebilir
- **Veri GizliliÄŸi:** KiÅŸisel veya hassas kullanÄ±cÄ± verilerini kazÄ±mayÄ±n
- **Sorumluluk:** Yazarlar, bu kodun yanlÄ±ÅŸ kullanÄ±mÄ±ndan veya kazÄ±ma aktivitelerinden kaynaklanan yasal sonuÃ§lardan **sorumlu deÄŸildir**

---

## ğŸ“Š Bilinen SÄ±nÄ±rlamalar ve Riskler

### Anti-Bot KorumasÄ±
BazÄ± TÃ¼rkÃ§e e-ticaret siteleri ÅŸunlarÄ± kullanÄ±r:
- Cloudflare korumasÄ±
- CAPTCHA zorluklarÄ±
- JavaScript tabanlÄ± iÃ§erik yÃ¼kleme

**Ã‡Ã¶zÃ¼mler:** Selenium headless modu, proxy rotasyonu, istek gecikmeleri, User-Agent rotasyonu

### Plugin GÃ¼venliÄŸi (ProdÃ¼ksiyon)
Custom plugin'ler arbitrer kod Ã§alÄ±ÅŸtÄ±rÄ±r. ProdÃ¼ksiyon iÃ§in:
- âœ… Plugin manifest'i doÄŸrulayÄ±n (metadata)
- âœ… Plugin baÄŸÄ±mlÄ±lÄ±klarÄ±nÄ± kontrol edin
- âœ… Plugin'leri izole konteynerler veya sandbox'larda Ã§alÄ±ÅŸtÄ±rÄ±n
- âœ… DaÄŸÄ±tÄ±mdan Ã¶nce kod incelemesi yapÄ±n

---

## ğŸ¤ KatkÄ±da Bulunma

KatkÄ±larÄ±nÄ±z memnuniyetle karÅŸÄ±lanÄ±r! LÃ¼tfen ÅŸu adÄ±mlarÄ± izleyin:

1. **Depoyu fork edin**
   ```bash
   git clone https://github.com/yourusername/ecommerce-scraper.git
   cd ecommerce-scraper
   ```

2. **Ã–zellik branch'i oluÅŸturun**
   ```bash
   git checkout -b feature/HarikaBirOzellik
   ```

3. **DeÄŸiÅŸiklikleri yapÄ±n ve commit edin**
   ```bash
   git commit -m "HarikaBirOzellik ekle"
   ```

4. **Branch'e push yapÄ±n**
   ```bash
   git push origin feature/HarikaBirOzellik
   ```

5. **GitHub'da Pull Request aÃ§Ä±n**

BÃ¼yÃ¼k deÄŸiÅŸiklikler iÃ§in lÃ¼tfen Ã¶nce Ã¶nerinizi tartÄ±ÅŸmak Ã¼zere bir issue aÃ§Ä±n.

---

## ğŸ“ Lisans

Bu proje **MIT LisansÄ±** altÄ±nda lisanslanmÄ±ÅŸtÄ±r. Detaylar iÃ§in [LICENSE](LICENSE) dosyasÄ±na bakÄ±n.

---

## ğŸ“ Ä°letiÅŸim ve Destek

**Yazar:** Hazar Ãœte  
**Email:** hazarute@gmail.com  
**GitHub:** [hazarute](https://github.com/hazarute)  
**Proje Linki:** [ecommerce-scraper](https://github.com/hazarute/ecommerce-scraper)

---

## ğŸ™ TeÅŸekkÃ¼rler ve Referanslar

- [Requests KÃ¼tÃ¼phanesi](https://docs.python-requests.org/) â€” HTTP istemcisi
- [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/) â€” HTML ayrÄ±ÅŸtÄ±rmasÄ±
- [Selenium](https://www.selenium.dev/) â€” TarayÄ±cÄ± otomasyonu
- [Streamlit](https://streamlit.io/) â€” UI framework'Ã¼
- Web kazÄ±ma eÄŸitimleri ve eÄŸitim kaynaklarÄ±

---

**Son GÃ¼ncelleme:** 14 KasÄ±m 2025 | Durum: Aktif GeliÅŸtirme (Streamlit GeÃ§iÅŸi Devam Ediyor)
